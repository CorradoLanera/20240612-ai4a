
@article{lanera_extending_2018,
	title = {Extending {PubMed} searches to {ClinicalTrials}.gov through a machine learning approach for systematic reviews},
	volume = {103},
	issn = {0895-4356, 1878-5921},
	url = {https://www.jclinepi.com/article/S0895-4356(18)30085-4/abstract#articleInformation},
	doi = {10.1016/j.jclinepi.2018.06.015},
	language = {English},
	urldate = {2024-06-12},
	journal = {Journal of Clinical Epidemiology},
	author = {Lanera, Corrado and Minto, Clara and Sharma, Abhinav and Gregori, Dario and Berchialla, Paola and Baldi, Ileana},
	month = nov,
	year = {2018},
	pmid = {29981872},
	note = {Publisher: Elsevier},
	keywords = {Clinical trial registry, Indexed search engine, Machine learning, Meta-analysis, Systematic review, Text mining},
	pages = {22--30},
}

@article{lanera_monitoring_2024,
	title = {Monitoring the {Epidemiology} of {Otitis} {Using} {Free}-{Text} {Pediatric} {Medical} {Notes}: {A} {Deep} {Learning} {Approach}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4426},
	shorttitle = {Monitoring the {Epidemiology} of {Otitis} {Using} {Free}-{Text} {Pediatric} {Medical} {Notes}},
	url = {https://www.mdpi.com/2075-4426/14/1/28},
	doi = {10.3390/jpm14010028},
	abstract = {Free-text information represents a valuable resource for epidemiological surveillance. Its unstructured nature, however, presents significant challenges in the extraction of meaningful information. This study presents a deep learning model for classifying otitis using pediatric medical records. We analyzed the Pedianet database, which includes data from January 2004 to August 2017. The model categorizes narratives from clinical record diagnoses into six types: no otitis, non-media otitis, non-acute otitis media (OM), acute OM (AOM), AOM with perforation, and recurrent AOM. Utilizing deep learning architectures, including an ensemble model, this study addressed the challenges associated with the manual classification of extensive narrative data. The performance of the model was evaluated according to a gold standard classification made by three expert clinicians. The ensemble model achieved values of 97.03, 93.97, 96.59, and 95.48 for balanced precision, balanced recall, accuracy, and balanced F1 measure, respectively. These results underscore the efficacy of using automated systems for medical diagnoses, especially in pediatric care. Our findings demonstrate the potential of deep learning in interpreting complex medical records, enhancing epidemiological surveillance and research. This approach offers significant improvements in handling large-scale medical data, ensuring accuracy and minimizing human error. The methodology is adaptable to other medical contexts, promising a new horizon in healthcare analytics.},
	language = {en},
	number = {1},
	urldate = {2024-06-12},
	journal = {Journal of Personalized Medicine},
	author = {Lanera, Corrado and Lorenzoni, Giulia and Barbieri, Elisa and Piras, Gianluca and Magge, Arjun and Weissenbacher, Davy and Donà, Daniele and Cantarutti, Luigi and Gonzalez-Hernandez, Graciela and Giaquinto, Carlo and Gregori, Dario},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {electronic medical record, machine learning, otitis, real-world data, text mining},
	pages = {28},
	file = {Full Text PDF:C\:\\Users\\corra\\Zotero\\storage\\TEIE5MKG\\Lanera et al. - 2024 - Monitoring the Epidemiology of Otitis Using Free-T.pdf:application/pdf},
}

@article{lanera_deep_2022,
	title = {A {Deep} {Learning} {Approach} to {Estimate} the {Incidence} of {Infectious} {Disease} {Cases} for {Routinely} {Collected} {Ambulatory} {Records}: {The} {Example} of {Varicella}-{Zoster}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1660-4601},
	shorttitle = {A {Deep} {Learning} {Approach} to {Estimate} the {Incidence} of {Infectious} {Disease} {Cases} for {Routinely} {Collected} {Ambulatory} {Records}},
	url = {https://www.mdpi.com/1660-4601/19/10/5959},
	doi = {10.3390/ijerph19105959},
	abstract = {The burden of infectious diseases is crucial for both epidemiological surveillance and prompt public health response. A variety of data, including textual sources, can be fruitfully exploited. Dealing with unstructured data necessitates the use of methods for automatic data-driven variable construction and machine learning techniques (MLT) show promising results. In this framework, varicella-zoster virus (VZV) infection was chosen to perform an automatic case identification with MLT. Pedianet, an Italian pediatric primary care database, was used to train a series of models to identify whether a child was diagnosed with VZV infection between 2004 and 2014 in the Veneto region, starting from free text fields. Given the nature of the task, a recurrent neural network (RNN) with bidirectional gated recurrent units (GRUs) was chosen; the same models were then used to predict the children’s status for the following years. A gold standard produced by manual extraction for the same interval was available for comparison. RNN-GRU improved its performance over time, reaching the maximum value of area under the ROC curve (AUC-ROC) of 95.30\% at the end of the period. The absolute bias in estimates of VZV infection was below 1.5\% in the last five years analyzed. The findings in this study could assist the large-scale use of EHRs for clinical outcome predictive modeling and help establish high-performance systems in other medical domains.},
	language = {en},
	number = {10},
	urldate = {2024-06-12},
	journal = {International Journal of Environmental Research and Public Health},
	author = {Lanera, Corrado and Baldi, Ileana and Francavilla, Andrea and Barbieri, Elisa and Tramontan, Lara and Scamarcia, Antonio and Cantarutti, Luigi and Giaquinto, Carlo and Gregori, Dario},
	month = jan,
	year = {2022},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, electronic health records, infectious disease, natural language processing, varicella-zoster},
	pages = {5959},
	file = {Full Text PDF:C\:\\Users\\corra\\Zotero\\storage\\S56GSSLT\\Lanera et al. - 2022 - A Deep Learning Approach to Estimate the Incidence.pdf:application/pdf},
}

@article{lorenzoni_use_2024,
	title = {Use of a {Large} {Language} {Model} to {Identify} and {Classify} {Injuries} {With} {Free}-{Text} {Emergency} {Department} {Data}},
	volume = {7},
	issn = {2574-3805},
	doi = {10.1001/jamanetworkopen.2024.13208},
	language = {eng},
	number = {5},
	journal = {JAMA network open},
	author = {Lorenzoni, Giulia and Gregori, Dario and Bressan, Silvia and Ocagli, Honoria and Azzolina, Danila and Da Dalt, Liviana and Berchialla, Paola},
	month = may,
	year = {2024},
	pmid = {38805230},
	pmcid = {PMC11134210},
	keywords = {Adult, Emergency Service, Hospital, Female, Humans, Male, Natural Language Processing, Wounds and Injuries},
	pages = {e2413208},
	file = {Full Text:C\:\\Users\\corra\\Zotero\\storage\\XBZ6KBUM\\Lorenzoni et al. - 2024 - Use of a Large Language Model to Identify and Clas.pdf:application/pdf},
}

@article{lanera_screening_2019,
	title = {Screening {PubMed} abstracts: {Is} class imbalance always a challenge to machine learning?},
	volume = {8},
	issn = {2046-4053},
	shorttitle = {Screening {PubMed} abstracts},
	doi = {10.1186/s13643-019-1245-8},
	abstract = {Background: The growing number of medical literature and textual data in online repositories led to an exponential increase in the workload of researchers involved in citation screening for systematic reviews. This work aims to combine machine learning techniques and data preprocessing for class imbalance to identify the outperforming strategy to screen articles in PubMed for inclusion in systematic reviews. Methods: We trained four binary text classifiers (support vector machines, k-nearest neighbor, random forest, and elastic-net regularized generalized linear models) in combination with four techniques for class imbalance: random undersampling and oversampling with 50:50 and 35:65 positive to negative class ratios and none as a benchmark. We used textual data of 14 systematic reviews as case studies. Difference between cross-validated area under the receiver operating characteristic curve (AUC-ROC) for machine learning techniques with and without preprocessing (delta AUC) was estimated within each systematic review, separately for each classifier. Meta-Analytic fixed-effect models were used to pool delta AUCs separately by classifier and strategy. Results: Cross-validated AUC-ROC for machine learning techniques (excluding k-nearest neighbor) without preprocessing was prevalently above 90\%. Except for k-nearest neighbor, machine learning techniques achieved the best improvement in conjunction with random oversampling 50:50 and random undersampling 35:65. Conclusions: Resampling techniques slightly improved the performance of the investigated machine learning techniques. From a computational perspective, random undersampling 35:65 may be preferred. © 2019 The Author(s).},
	language = {English},
	number = {1},
	journal = {Systematic Reviews},
	author = {Lanera, C. and Berchialla, P. and Sharma, A. and Minto, C. and Gregori, D. and Baldi, I.},
	year = {2019},
	keywords = {Classification, Indexed search engine, Machine learning, Systematic review, Text mining, Unbalanced data},
	annote = {Cited By :14},
	file = {Full Text:C\:\\Users\\corra\\Zotero\\storage\\QW5GX6QE\\Lanera et al. - 2019 - Screening PubMed abstracts Is class imbalance alw.pdf:application/pdf;Snapshot:C\:\\Users\\corra\\Zotero\\storage\\M72AFXMX\\display.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\corra\\Zotero\\storage\\9DARSEI5\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\corra\\Zotero\\storage\\HHYFGDYA\\1706.html:text/html},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages; updated authors list; fixed author names and added citation},
	file = {arXiv Fulltext PDF:C\:\\Users\\corra\\Zotero\\storage\\T5CNEP4D\\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\corra\\Zotero\\storage\\RPI5DBLX\\2303.html:text/html},
}
